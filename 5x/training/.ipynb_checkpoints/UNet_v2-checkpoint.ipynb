{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a U-Net\n",
    "#### Train a Hamamtsu HCC U-Net from the scratch.\n",
    "#### Grayscale image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 classes: binary_crossentropy, sigmoid, mask(0, 255), flag_multi_class=False (chosen)\n",
    "\n",
    "multi classes(2 included) : catocaries_crossentropy, softmax, mask(0, 1, 2, ...), flag_multi_class=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Input, Model\n",
    "from keras.layers import Conv2D, Concatenate, MaxPooling2D\n",
    "from keras.layers import UpSampling2D, Dropout, BatchNormalization\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.callbacks import CSVLogger, ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "'''\n",
    "U-Net: Convolutional Networks for Biomedical Image Segmentation\n",
    "(https://arxiv.org/abs/1505.04597)\n",
    "---\n",
    "img_shape: (height, width, channels)\n",
    "out_ch: number of output channels\n",
    "start_ch: number of channels of the first conv\n",
    "depth: zero indexed depth of the U-structure\n",
    "inc_rate: rate at which the conv channels will increase\n",
    "activation: activation function after convolutions\n",
    "dropout: amount of dropout in the contracting part\n",
    "batchnorm: adds Batch Normalization if true\n",
    "maxpool: use strided conv instead of maxpooling if false\n",
    "upconv: use transposed conv instead of upsamping + conv if false\n",
    "residual: add residual connections around each conv block if true\n",
    "'''\n",
    "\n",
    "def conv_block(m, dim, acti, bn, res, do=0):\n",
    "\tn = Conv2D(dim, 3, activation=acti, padding='same')(m)\n",
    "\tn = BatchNormalization()(n) if bn else n\n",
    "\tn = Dropout(do)(n) if do else n\n",
    "\tn = Conv2D(dim, 3, activation=acti, padding='same')(n)\n",
    "\tn = BatchNormalization()(n) if bn else n\n",
    "\treturn Concatenate()([m, n]) if res else n\n",
    "\n",
    "def level_block(m, dim, depth, inc, acti, do, bn, mp, up, res):\n",
    "\tif depth > 0:\n",
    "\t\tn = conv_block(m, dim, acti, bn, res)\n",
    "\t\tm = MaxPooling2D()(n) if mp else Conv2D(dim, 3, strides=2, padding='same')(n)\n",
    "\t\tm = level_block(m, int(inc*dim), depth-1, inc, acti, do, bn, mp, up, res)\n",
    "\t\tif up:\n",
    "\t\t\tm = UpSampling2D()(m)\n",
    "\t\t\tm = Conv2D(dim, 2, activation=acti, padding='same')(m)\n",
    "\t\telse:\n",
    "\t\t\tm = Conv2DTranspose(dim, 3, strides=2, activation=acti, padding='same')(m)\n",
    "\t\tn = Concatenate()([n, m])\n",
    "\t\tm = conv_block(n, dim, acti, bn, res)\n",
    "\telse:\n",
    "\t\tm = conv_block(m, dim, acti, bn, res, do)\n",
    "\treturn m\n",
    "\n",
    "def UNet(img_shape, out_ch=1, start_ch=64, depth=4, inc_rate=2., activation='relu', \n",
    "\t\t dropout=0.5, batchnorm=False, maxpool=True, upconv=True, residual=False):\n",
    "\ti = Input(shape=img_shape)\n",
    "\to = level_block(i, start_ch, depth, inc_rate, activation, dropout, batchnorm, maxpool, upconv, residual)\n",
    "\to = Conv2D(out_ch, 1, activation='sigmoid')(o)\n",
    "\treturn Model(inputs=i, outputs=o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet((256,256,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 256, 256, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 256, 256, 64) 640         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 256, 256, 64) 36928       conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 128, 128, 64) 0           conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 128, 128, 128 73856       max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 128, 128, 128 147584      conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 64, 64, 128)  0           conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 64, 64, 256)  295168      max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 64, 64, 256)  590080      conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2D)  (None, 32, 32, 256)  0           conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 32, 32, 512)  1180160     max_pooling2d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 32, 32, 512)  2359808     conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2D)  (None, 16, 16, 512)  0           conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 16, 16, 1024) 4719616     max_pooling2d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 16, 16, 1024) 0           conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 16, 16, 1024) 9438208     dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_5 (UpSampling2D)  (None, 32, 32, 1024) 0           conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 32, 32, 512)  2097664     up_sampling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 32, 32, 1024) 0           conv2d_31[0][0]                  \n",
      "                                                                 conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 32, 32, 512)  4719104     concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 32, 32, 512)  2359808     conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_6 (UpSampling2D)  (None, 64, 64, 512)  0           conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 64, 64, 256)  524544      up_sampling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 64, 64, 512)  0           conv2d_29[0][0]                  \n",
      "                                                                 conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 64, 64, 256)  1179904     concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 64, 64, 256)  590080      conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_7 (UpSampling2D)  (None, 128, 128, 256 0           conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 128, 128, 128 131200      up_sampling2d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 128, 128, 256 0           conv2d_27[0][0]                  \n",
      "                                                                 conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 128, 128, 128 295040      concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 128, 128, 128 147584      conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_8 (UpSampling2D)  (None, 256, 256, 128 0           conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 256, 256, 64) 32832       up_sampling2d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 256, 256, 128 0           conv2d_25[0][0]                  \n",
      "                                                                 conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 256, 256, 64) 73792       concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 256, 256, 64) 36928       conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 256, 256, 1)  65          conv2d_45[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 31,030,593\n",
      "Trainable params: 31,030,593\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(optimizer = SGD(lr = 1e-2, decay=0.001, momentum=0.9), loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(optimizer = Adam(lr = 1e-3), loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = Adam(lr = 1e-3), loss = 'jaccard_distance', metrics = ['accuracy']) #to be checked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjustData(img,mask,flag_multi_class,num_class):\n",
    "    if(flag_multi_class):\n",
    "        img = img / 255\n",
    "        mask = mask[:,:,:,0] if(len(mask.shape) == 4) else mask[:,:,0]\n",
    "        new_mask = np.zeros(mask.shape + (num_class,))\n",
    "        for i in range(num_class):\n",
    "            #for one pixel in the image, find the class in mask and convert it into one-hot vector\n",
    "            #index = np.where(mask == i)\n",
    "            #index_mask = (index[0],index[1],index[2],np.zeros(len(index[0]),dtype = np.int64) + i) if (len(mask.shape) == 4) else (index[0],index[1],np.zeros(len(index[0]),dtype = np.int64) + i)\n",
    "            #new_mask[index_mask] = 1\n",
    "            new_mask[mask == i,i] = 1\n",
    "        new_mask = np.reshape(new_mask,(new_mask.shape[0],new_mask.shape[1]*new_mask.shape[2],new_mask.shape[3])) if flag_multi_class else np.reshape(new_mask,(new_mask.shape[0]*new_mask.shape[1],new_mask.shape[2]))\n",
    "        mask = new_mask\n",
    "    elif(np.max(img) > 1):\n",
    "        img = img / 255\n",
    "        mask = mask /255\n",
    "#         mask[mask > 0.5] = 1\n",
    "#         mask[mask <= 0.5] = 0\n",
    "    return (img,mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainGenerator(batch_size,train_path,image_folder,mask_folder,aug_dict,image_color_mode = \"grayscale\",\n",
    "                    mask_color_mode = \"grayscale\",image_save_prefix  = \"image\",mask_save_prefix  = \"mask\",\n",
    "                    flag_multi_class = False,num_class = 2,save_to_dir = None,target_size = (256,256),seed = 1):\n",
    "    '''\n",
    "    can generate image and mask at the same time\n",
    "    use the same seed for image_datagen and mask_datagen to ensure the transformation for image and mask is the same\n",
    "    if you want to visualize the results of generator, set save_to_dir = \"your path\"\n",
    "    '''\n",
    "    image_datagen = ImageDataGenerator(**aug_dict)\n",
    "    mask_datagen = ImageDataGenerator(**aug_dict)\n",
    "    image_generator = image_datagen.flow_from_directory(\n",
    "        train_path,\n",
    "        classes = [image_folder],\n",
    "        class_mode = None,\n",
    "        color_mode = image_color_mode,\n",
    "        target_size = target_size,\n",
    "        batch_size = batch_size,\n",
    "        save_to_dir = save_to_dir,\n",
    "        save_prefix  = image_save_prefix,\n",
    "        seed = seed)\n",
    "    mask_generator = mask_datagen.flow_from_directory(\n",
    "        train_path,\n",
    "        classes = [mask_folder],\n",
    "        class_mode = None,\n",
    "        color_mode = mask_color_mode,\n",
    "        target_size = target_size,\n",
    "        batch_size = batch_size,\n",
    "        save_to_dir = save_to_dir,\n",
    "        save_prefix  = mask_save_prefix,\n",
    "        seed = seed)\n",
    "    train_generator = zip(image_generator, mask_generator)\n",
    "    for (img,mask) in train_generator:\n",
    "        img,mask = adjustData(img,mask,flag_multi_class,num_class)\n",
    "        yield (img,mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import trainGenerator\n",
    "# from keras.data import trainGenerator\n",
    "\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "\n",
    "data_gen_args = dict(featurewise_std_normalization=True)\n",
    "\n",
    "train_generator = trainGenerator(batch_size=16, train_path='E:\\\\deeplearning\\\\Hepatocarcinomes\\\\data\\\\5x\\\\training\\\\split64_image', image_folder='image_unet_train', mask_folder='mask_unet_train', aug_dict=data_gen_args, target_size = (256,256),  seed = 1)\n",
    "validation_generator = trainGenerator(batch_size=16, train_path='E:\\\\deeplearning\\\\Hepatocarcinomes\\\\data\\\\5x\\\\training\\\\split64_image', image_folder='image_unet_val', mask_folder='mask_unet_val', aug_dict=data_gen_args, target_size = (256,256),  seed = 1)\n",
    "\n",
    "csv_logger = CSVLogger('E:\\\\deeplearning\\\\Hepatocarcinomes\\\\models\\\\5x\\\\unet\\\\jac_training.csv', append=True)\n",
    "model_checkpoint = ModelCheckpoint('E:\\\\deeplearning\\\\Hepatocarcinomes\\\\models\\\\5x\\\\unet\\\\jac_weights.{epoch:02d}-{val_acc:.4f}.hdf5', monitor='val_loss',verbose=1, save_best_only=True, period=5)\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=15, verbose=1, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, min_delta=0.01, cooldown=3, patience=9, min_lr=0.00001, verbose=1)\n",
    "results = model.fit_generator(train_generator, steps_per_epoch=343,epochs=100, validation_data=validation_generator, validation_steps=68, callbacks=[csv_logger, model_checkpoint, early_stop, reduce_lr])\n",
    "csv_logger.csv_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('E:\\\\deeplearning\\\\Hepatocarcinomes\\\\models\\\\5x\\\\unet\\\\jac_best_model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # =============================================================================\n",
    "# # Import images\n",
    "# # =============================================================================\n",
    "# seed = 42\n",
    "# random.seed = seed\n",
    "# np.random.seed = seed\n",
    "\n",
    "# trainPath=\".\\\\input\\\\train\"\n",
    "# fileList=[os.path.join(trainPath,fname) for fname in os.listdir(trainPath)]\n",
    "# fileList=fileList[0:-1] #a cause fichier thumbnail qui fout la merde.\n",
    "\n",
    "# #load train imgs and masks\n",
    "# trainList=[]\n",
    "# trainMask=[]\n",
    "# for file in fileList:\n",
    "#     if file.find(\"tiff\")==-1:\n",
    "#         trainList.append(file)\n",
    "#     else:\n",
    "#         trainMask.append(file)\n",
    "            \n",
    "# x_train=np.array((len(trainList),64,64,3),dtype='uint8')\n",
    "# y_train=np.array((len(trainList),64,64,1),dtype='bool')\n",
    "# x_train = np.array([np.array(cv2.imread(fname)) for fname in trainList])\n",
    "# #should be n,x,y,c false/true\n",
    "# y_train = np.array([np.array(cv2.imread(fname,0)).astype('bool') for fname in trainMask])\n",
    "# y_train= np.reshape(y_train,(77,256,256,1))\n",
    "\n",
    "# #load validation images and masks\n",
    "# valPath=\".\\\\input\\\\val\"\n",
    "# fileList=[os.path.join(valPath,fname) for fname in os.listdir(valPath)]\n",
    "# fileList=fileList[0:-1] #a cause fichier thumbnail qui fout la merde\n",
    "# valList=[]\n",
    "# valMask=[]\n",
    "# for file in fileList:\n",
    "#     if file.find(\"mask\")==-1:\n",
    "#         valList.append(file)\n",
    "#     else:\n",
    "#         valMask.append(file)\n",
    "            \n",
    "# x_val=np.array((len(valMask),256,256,3),dtype='uint8')\n",
    "# y_val=np.array((len(valMask),256,256,1),dtype='bool')\n",
    "# x_val = np.array([np.array(cv2.imread(fname)) for fname in valList])\n",
    "# y_val = np.array([np.array(cv2.imread(fname,0)).astype('bool') for fname in valMask])\n",
    "# y_val=np.reshape(y_val,(20,256,256,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Training\n",
    "# =============================================================================\n",
    "# callbacks_list = [\n",
    "#         callbacks.EarlyStopping(\n",
    "#                 monitor='val_acc',\n",
    "#                 patience=20,\n",
    "#                 verbose=1\n",
    "#                 ),\n",
    "                \n",
    "#         callbacks.ModelCheckpoint(\n",
    "#                 filepath='Unet_baseline.h5',\n",
    "#                 monitor='val_acc',\n",
    "#                 save_best_only=True,\n",
    "#                 verbose=1\n",
    "#                 ),\n",
    "                \n",
    "#         callbacks.ReduceLROnPlateau(\n",
    "#                 monitor='val_acc',\n",
    "#                 factor=0.1,\n",
    "#                 patience=10,\n",
    "#                 verbose=1\n",
    "#                 )\n",
    "        \n",
    "#                 ]\n",
    "\n",
    "# results = model.fit(x_train, \n",
    "#                     y_train, \n",
    "#                     validation_split=0.2, \n",
    "#                     batch_size=16, \n",
    "#                     epochs=100,\n",
    "#                     callbacks=callbacks_list)\n",
    "\n",
    "#plot training results\n",
    "acc=results.history['acc']\n",
    "val_acc=results.history['val_acc']\n",
    "loss=results.history['loss']\n",
    "val_loss=results.history['val_loss']\n",
    "\n",
    "epochs=range(1, len(acc)+1)\n",
    "plt.figure()\n",
    "plt.xlabel('epochs')\n",
    "plt.plot(epochs,acc,'b',label='Training acc')\n",
    "plt.plot(epochs,val_acc,'',label='Validation acc') \n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.savefig('E:\\\\deeplearning\\\\Hepatocarcinomes\\\\models\\\\5x\\\\unet\\\\jac_train_val_acc')\n",
    "plt.figure()\n",
    "plt.xlabel('epochs')\n",
    "plt.plot(epochs,loss,'b',label='Training loss')\n",
    "plt.plot(epochs, val_loss,'',label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.savefig('E:\\\\deeplearning\\\\Hepatocarcinomes\\\\models\\\\5x\\\\unet\\\\jac_train_val_loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import os\n",
    "# def testGenerator(test_path,num_image = 7,target_size = (256,256),flag_multi_class = False):\n",
    "#     for i in range(num_image):\n",
    "#         img = cv2.imread(os.path.join(test_path,\"%d.tif\"%i),0)\n",
    "        \n",
    "#         img = img / 255\n",
    "#         img = cv2.resize(img,target_size)\n",
    "# #         print(img.shape)\n",
    "#         img = np.reshape(img,img.shape+(1,)) if (not flag_multi_class) else img\n",
    "#         img = np.reshape(img,(1,)+img.shape)\n",
    "# #         print(img.shape)\n",
    "#         yield img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nor = [0,0,0]\n",
    "# tum = [255,255,255]\n",
    "\n",
    "# COLOR_DICT = np.array([nor, tum])\n",
    "\n",
    "# def labelVisualize(num_class,color_dict,img):\n",
    "#     img = img[:,:,0] if len(img.shape) == 3 else img\n",
    "#     img_out = np.zeros(img.shape + (3,))\n",
    "#     for i in range(num_class):\n",
    "#         img_out[img == i,:] = color_dict[i]\n",
    "#     return img_out\n",
    "\n",
    "\n",
    "# def saveResult(save_path,npyfile,flag_multi_class = False,num_class = 2):\n",
    "#     for i,item in enumerate(npyfile):\n",
    "#         img = labelVisualize(num_class,COLOR_DICT,item) if flag_multi_class else item[:,:,0]\n",
    "#         img = cv2.resize(img, (64, 64))\n",
    "#         cv2.imwrite(os.path.join(save_path,\"%d_predict.tiff\"%i),img*255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testGene = testGenerator(\"E:\\\\deeplearning\\\\Hepatocarcinomes\\\\data\\\\5x\\\\training\\\\split64_image\\\\test\", target_size = (256,256))\n",
    "# results_test = model.predict_generator(testGene,7,verbose=1)\n",
    "# # print(sum(results[:,:,:,:]!=0))\n",
    "# # for i in range(7):\n",
    "# #     cv2.imwrite(\"E:\\\\deeplearning\\\\Hepatocarcinomes\\\\data\\\\5x\\\\training\\\\split64_image\\\\test\"+'\\\\'+str(i)+'.tiff',results[i]*255)\n",
    "# saveResult(\"E:\\\\deeplearning\\\\Hepatocarcinomes\\\\data\\\\5x\\\\training\\\\split64_image\\\\test\",results_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python r-tensorflow",
   "language": "python",
   "name": "r-tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
