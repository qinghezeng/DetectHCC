{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a U-Net (Test arch, image type)\n",
    "#### Train a Hamamtsu HCC U-Net from the scratch.\n",
    "#### Grayscale image.\n",
    "#### Add a dense layer at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 classes: binary_crossentropy, sigmoid, mask(0, 255), flag_multi_class=False\n",
    "\n",
    "multi classes (2 included) : catocaries_crossentropy, softmax, mask(0, 1, 2, ...), flag_multi_class=True (chosen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Input, Model\n",
    "from keras.layers import Conv2D, Concatenate, MaxPooling2D\n",
    "from keras.layers import UpSampling2D, Dropout, BatchNormalization, Dense, Flatten\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.callbacks import CSVLogger, ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "'''\n",
    "U-Net: Convolutional Networks for Biomedical Image Segmentation\n",
    "(https://arxiv.org/abs/1505.04597)\n",
    "---\n",
    "img_shape: (height, width, channels)\n",
    "out_ch: number of output channels\n",
    "start_ch: number of channels of the first conv\n",
    "depth: zero indexed depth of the U-structure\n",
    "inc_rate: rate at which the conv channels will increase\n",
    "activation: activation function after convolutions\n",
    "dropout: amount of dropout in the contracting part\n",
    "batchnorm: adds Batch Normalization if true\n",
    "maxpool: use strided conv instead of maxpooling if false\n",
    "upconv: use transposed conv instead of upsamping + conv if false\n",
    "residual: add residual connections around each conv block if true\n",
    "'''\n",
    "\n",
    "def conv_block(m, dim, acti, bn, res, do=0):\n",
    "\tn = Conv2D(dim, 3, activation=acti, padding='same')(m)\n",
    "\tn = BatchNormalization()(n) if bn else n\n",
    "\tn = Dropout(do)(n) if do else n\n",
    "\tn = Conv2D(dim, 3, activation=acti, padding='same')(n)\n",
    "\tn = BatchNormalization()(n) if bn else n\n",
    "\treturn Concatenate()([m, n]) if res else n\n",
    "\n",
    "def level_block(m, dim, depth, inc, acti, do, bn, mp, up, res):\n",
    "\tif depth > 0:\n",
    "\t\tn = conv_block(m, dim, acti, bn, res)\n",
    "\t\tm = MaxPooling2D()(n) if mp else Conv2D(dim, 3, strides=2, padding='same')(n)\n",
    "\t\tm = level_block(m, int(inc*dim), depth-1, inc, acti, do, bn, mp, up, res)\n",
    "\t\tif up:\n",
    "\t\t\tm = UpSampling2D()(m)\n",
    "\t\t\tm = Conv2D(dim, 2, activation=acti, padding='same')(m)\n",
    "\t\telse:\n",
    "\t\t\tm = Conv2DTranspose(dim, 3, strides=2, activation=acti, padding='same')(m)\n",
    "\t\tn = Concatenate()([n, m])\n",
    "\t\tm = conv_block(n, dim, acti, bn, res)\n",
    "\telse:\n",
    "\t\tm = conv_block(m, dim, acti, bn, res, do)\n",
    "\treturn m\n",
    "\n",
    "def UNet(img_shape, out_ch=1, start_ch=64, depth=4, inc_rate=2., activation='relu', \n",
    "\t\t dropout=0.5, batchnorm=False, maxpool=True, upconv=True, residual=False):\n",
    "\ti = Input(shape=img_shape)\n",
    "\to = level_block(i, start_ch, depth, inc_rate, activation, dropout, batchnorm, maxpool, upconv, residual)\n",
    "# \to = Conv2D(out_ch, 1, activation='sigmoid')(o)\n",
    "\to = Conv2D(2, 1, activation=activation)(o)\n",
    "\to = Flatten()(o)\n",
    "\to = Dense(out_ch, use_bias=False, activation='sigmoid')(o)\n",
    "\treturn Model(inputs=i, outputs=o)\n",
    "#add a dense layer, modify grayscale to rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet((256,256,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 256, 256, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 256, 256, 64) 640         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 256, 256, 64) 36928       conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2D)  (None, 128, 128, 64) 0           conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 128, 128, 128 73856       max_pooling2d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 128, 128, 128 147584      conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling2D) (None, 64, 64, 128)  0           conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 64, 64, 256)  295168      max_pooling2d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 64, 64, 256)  590080      conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling2D) (None, 32, 32, 256)  0           conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, 32, 32, 512)  1180160     max_pooling2d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, 32, 32, 512)  2359808     conv2d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling2D) (None, 16, 16, 512)  0           conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, 16, 16, 1024) 4719616     max_pooling2d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 16, 16, 1024) 0           conv2d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, 16, 16, 1024) 9438208     dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_9 (UpSampling2D)  (None, 32, 32, 1024) 0           conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 32, 32, 512)  2097664     up_sampling2d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 32, 32, 1024) 0           conv2d_54[0][0]                  \n",
      "                                                                 conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, 32, 32, 512)  4719104     concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, 32, 32, 512)  2359808     conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_10 (UpSampling2D) (None, 64, 64, 512)  0           conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, 64, 64, 256)  524544      up_sampling2d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 64, 64, 512)  0           conv2d_52[0][0]                  \n",
      "                                                                 conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, 64, 64, 256)  1179904     concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, 64, 64, 256)  590080      conv2d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_11 (UpSampling2D) (None, 128, 128, 256 0           conv2d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, 128, 128, 128 131200      up_sampling2d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 128, 128, 256 0           conv2d_50[0][0]                  \n",
      "                                                                 conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, 128, 128, 128 295040      concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, 128, 128, 128 147584      conv2d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_12 (UpSampling2D) (None, 256, 256, 128 0           conv2d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, 256, 256, 64) 32832       up_sampling2d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, 256, 256, 128 0           conv2d_48[0][0]                  \n",
      "                                                                 conv2d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, 256, 256, 64) 73792       concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)              (None, 256, 256, 64) 36928       conv2d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)              (None, 256, 256, 2)  130         conv2d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 131072)       0           conv2d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 2)            262144      flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 31,292,802\n",
      "Trainable params: 31,292,802\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(optimizer = SGD(lr = 1e-2, decay=0.001, momentum=0.9), loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = Adam(lr = 1e-3), loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(optimizer = Adam(lr = 1e-3), loss = 'jaccard_distance', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjustData(img,mask,flag_multi_class,num_class):\n",
    "    if(flag_multi_class):\n",
    "        img = img / 255\n",
    "        mask = mask[:,:,:,0] if(len(mask.shape) == 4) else mask[:,:,0]\n",
    "        new_mask = np.zeros(mask.shape + (num_class,))\n",
    "        for i in range(num_class):\n",
    "#             for one pixel in the image, find the class in mask and convert it into one-hot vector\n",
    "            index = np.where(mask == i)\n",
    "            index_mask = (index[0],index[1],index[2],np.zeros(len(index[0]),dtype = np.int64) + i) if (len(mask.shape) == 4) else (index[0],index[1],np.zeros(len(index[0]),dtype = np.int64) + i)\n",
    "            new_mask[index_mask] = 1\n",
    "            new_mask[mask == i,i] = 1\n",
    "        new_mask = np.reshape(new_mask,(new_mask.shape[0],new_mask.shape[1]*new_mask.shape[2],new_mask.shape[3])) if flag_multi_class else np.reshape(new_mask,(new_mask.shape[0]*new_mask.shape[1],new_mask.shape[2]))\n",
    "        mask = new_mask\n",
    "    elif(np.max(img) > 1):\n",
    "        img = img / 255\n",
    "        mask = mask /255\n",
    "#         mask[mask > 0.5] = 1\n",
    "#         mask[mask <= 0.5] = 0\n",
    "    return (img,mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainGenerator(batch_size,train_path,image_folder,mask_folder,aug_dict,image_color_mode = \"grayscale\",\n",
    "                    mask_color_mode = \"grayscale\",image_save_prefix  = \"image\",mask_save_prefix  = \"mask\",\n",
    "                    flag_multi_class = False,num_class = 2,save_to_dir = None,target_size = (256,256),seed = 1):\n",
    "    '''\n",
    "    can generate image and mask at the same time\n",
    "    use the same seed for image_datagen and mask_datagen to ensure the transformation for image and mask is the same\n",
    "    if you want to visualize the results of generator, set save_to_dir = \"your path\"\n",
    "    '''\n",
    "    image_datagen = ImageDataGenerator(**aug_dict)\n",
    "    mask_datagen = ImageDataGenerator(**aug_dict)\n",
    "    image_generator = image_datagen.flow_from_directory(\n",
    "        train_path,\n",
    "        classes = [image_folder],\n",
    "        class_mode = None,\n",
    "        color_mode = image_color_mode,\n",
    "        target_size = target_size,\n",
    "        batch_size = batch_size,\n",
    "        save_to_dir = save_to_dir,\n",
    "        save_prefix  = image_save_prefix,\n",
    "        seed = seed)\n",
    "    mask_generator = mask_datagen.flow_from_directory(\n",
    "        train_path,\n",
    "        classes = [mask_folder],\n",
    "        class_mode = None,\n",
    "        color_mode = mask_color_mode,\n",
    "        target_size = target_size,\n",
    "        batch_size = batch_size,\n",
    "        save_to_dir = save_to_dir,\n",
    "        save_prefix  = mask_save_prefix,\n",
    "        seed = seed)\n",
    "    train_generator = zip(image_generator, mask_generator)\n",
    "    for (img,mask) in train_generator:\n",
    "        img,mask = adjustData(img,mask,flag_multi_class,num_class)\n",
    "        yield (img,mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "Found 1100 images belonging to 1 classes.\n",
      "Found 5500 images belonging to 1 classes.\n",
      "Found 1100 images belonging to 1 classes.\n",
      "Found 5500 images belonging to 1 classes.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_2 to have 2 dimensions, but got array with shape (16, 65536, 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-4ea7198a2f05>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mearly_stop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'val_loss'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_delta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrestore_best_weights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mreduce_lr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mReduceLROnPlateau\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'val_loss'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfactor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_delta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcooldown\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_lr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.00001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m343\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m68\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcsv_logger\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_checkpoint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mearly_stop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce_lr\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[0mcsv_logger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Anaconda3\\envs\\r-tensorflow\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Anaconda3\\envs\\r-tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1418\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1420\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Anaconda3\\envs\\r-tensorflow\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[0;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m                                             class_weight=class_weight)\n\u001b[0m\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Anaconda3\\envs\\r-tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1209\u001b[0m             \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1210\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1211\u001b[1;33m             class_weight=class_weight)\n\u001b[0m\u001b[0;32m   1212\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1213\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1.\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Anaconda3\\envs\\r-tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    787\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 789\u001b[1;33m                 exception_prefix='target')\n\u001b[0m\u001b[0;32m    790\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m             \u001b[1;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Anaconda3\\envs\\r-tensorflow\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    126\u001b[0m                         \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' dimensions, but got array '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[0;32m    129\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking target: expected dense_2 to have 2 dimensions, but got array with shape (16, 65536, 2)"
     ]
    }
   ],
   "source": [
    "# from keras.models import trainGenerator\n",
    "# from keras.data import trainGenerator\n",
    "\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "\n",
    "data_gen_args = dict(featurewise_std_normalization=True)\n",
    "\n",
    "train_generator = trainGenerator(batch_size=16, train_path='E:\\\\deeplearning\\\\Hepatocarcinomes\\\\data\\\\5x\\\\training\\\\split64_image', image_folder='image_unet_train', mask_folder='mask_unet_train', aug_dict=data_gen_args, target_size=(256,256),  seed = 1, flag_multi_class=False)\n",
    "validation_generator = trainGenerator(batch_size=16, train_path='E:\\\\deeplearning\\\\Hepatocarcinomes\\\\data\\\\5x\\\\training\\\\split64_image', image_folder='image_unet_val', mask_folder='mask_unet_val', aug_dict=data_gen_args, target_size=(256,256),  seed = 1, flag_multi_class=False)\n",
    "\n",
    "csv_logger = CSVLogger('E:\\\\deeplearning\\\\Hepatocarcinomes\\\\models\\\\5x\\\\unet\\\\dense_lr3_16_training.csv', append=True)\n",
    "model_checkpoint = ModelCheckpoint('E:\\\\deeplearning\\\\Hepatocarcinomes\\\\models\\\\5x\\\\unet\\\\dense_lr3_16_weights.{epoch:02d}-{val_acc:.4f}.hdf5', monitor='val_loss',verbose=1, save_best_only=True, period=5)\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=15, verbose=1, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, min_delta=0.01, cooldown=3, patience=9, min_lr=0.00001, verbose=1)\n",
    "results = model.fit_generator(train_generator, steps_per_epoch=343,epochs=100, validation_data=validation_generator, validation_steps=68, callbacks=[csv_logger, model_checkpoint, early_stop, reduce_lr])\n",
    "csv_logger.csv_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('E:\\\\deeplearning\\\\Hepatocarcinomes\\\\models\\\\5x\\\\unet\\\\dense_lr3_16_best_model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # =============================================================================\n",
    "# # Import images\n",
    "# # =============================================================================\n",
    "# seed = 42\n",
    "# random.seed = seed\n",
    "# np.random.seed = seed\n",
    "\n",
    "# trainPath=\".\\\\input\\\\train\"\n",
    "# fileList=[os.path.join(trainPath,fname) for fname in os.listdir(trainPath)]\n",
    "# fileList=fileList[0:-1] #a cause fichier thumbnail qui fout la merde.\n",
    "\n",
    "# #load train imgs and masks\n",
    "# trainList=[]\n",
    "# trainMask=[]\n",
    "# for file in fileList:\n",
    "#     if file.find(\"tiff\")==-1:\n",
    "#         trainList.append(file)\n",
    "#     else:\n",
    "#         trainMask.append(file)\n",
    "            \n",
    "# x_train=np.array((len(trainList),64,64,3),dtype='uint8')\n",
    "# y_train=np.array((len(trainList),64,64,1),dtype='bool')\n",
    "# x_train = np.array([np.array(cv2.imread(fname)) for fname in trainList])\n",
    "# #should be n,x,y,c false/true\n",
    "# y_train = np.array([np.array(cv2.imread(fname,0)).astype('bool') for fname in trainMask])\n",
    "# y_train= np.reshape(y_train,(77,256,256,1))\n",
    "\n",
    "# #load validation images and masks\n",
    "# valPath=\".\\\\input\\\\val\"\n",
    "# fileList=[os.path.join(valPath,fname) for fname in os.listdir(valPath)]\n",
    "# fileList=fileList[0:-1] #a cause fichier thumbnail qui fout la merde\n",
    "# valList=[]\n",
    "# valMask=[]\n",
    "# for file in fileList:\n",
    "#     if file.find(\"mask\")==-1:\n",
    "#         valList.append(file)\n",
    "#     else:\n",
    "#         valMask.append(file)\n",
    "            \n",
    "# x_val=np.array((len(valMask),256,256,3),dtype='uint8')\n",
    "# y_val=np.array((len(valMask),256,256,1),dtype='bool')\n",
    "# x_val = np.array([np.array(cv2.imread(fname)) for fname in valList])\n",
    "# y_val = np.array([np.array(cv2.imread(fname,0)).astype('bool') for fname in valMask])\n",
    "# y_val=np.reshape(y_val,(20,256,256,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Training\n",
    "# =============================================================================\n",
    "# callbacks_list = [\n",
    "#         callbacks.EarlyStopping(\n",
    "#                 monitor='val_acc',\n",
    "#                 patience=20,\n",
    "#                 verbose=1\n",
    "#                 ),\n",
    "                \n",
    "#         callbacks.ModelCheckpoint(\n",
    "#                 filepath='Unet_baseline.h5',\n",
    "#                 monitor='val_acc',\n",
    "#                 save_best_only=True,\n",
    "#                 verbose=1\n",
    "#                 ),\n",
    "                \n",
    "#         callbacks.ReduceLROnPlateau(\n",
    "#                 monitor='val_acc',\n",
    "#                 factor=0.1,\n",
    "#                 patience=10,\n",
    "#                 verbose=1\n",
    "#                 )\n",
    "        \n",
    "#                 ]\n",
    "\n",
    "# results = model.fit(x_train, \n",
    "#                     y_train, \n",
    "#                     validation_split=0.2, \n",
    "#                     batch_size=16, \n",
    "#                     epochs=100,\n",
    "#                     callbacks=callbacks_list)\n",
    "\n",
    "#plot training results\n",
    "acc=results.history['acc']\n",
    "val_acc=results.history['val_acc']\n",
    "loss=results.history['loss']\n",
    "val_loss=results.history['val_loss']\n",
    "\n",
    "epochs=range(1, len(acc)+1)\n",
    "plt.figure()\n",
    "plt.xlabel('epochs')\n",
    "plt.plot(epochs,acc,'b',label='Training acc')\n",
    "plt.plot(epochs,val_acc,'',label='Validation acc') \n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.savefig('E:\\\\deeplearning\\\\Hepatocarcinomes\\\\models\\\\5x\\\\unet\\\\dense_lr3_16_train_val_acc')\n",
    "plt.figure()\n",
    "plt.xlabel('epochs')\n",
    "plt.plot(epochs,loss,'b',label='Training loss')\n",
    "plt.plot(epochs, val_loss,'',label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.savefig('E:\\\\deeplearning\\\\Hepatocarcinomes\\\\models\\\\5x\\\\unet\\\\dense_lr3_16train_val_loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import os\n",
    "# def testGenerator(test_path,num_image = 7,target_size = (256,256),flag_multi_class = False):\n",
    "#     for i in range(num_image):\n",
    "#         img = cv2.imread(os.path.join(test_path,\"%d.tif\"%i),0)\n",
    "        \n",
    "#         img = img / 255\n",
    "#         img = cv2.resize(img,target_size)\n",
    "# #         print(img.shape)\n",
    "#         img = np.reshape(img,img.shape+(1,)) if (not flag_multi_class) else img\n",
    "#         img = np.reshape(img,(1,)+img.shape)\n",
    "# #         print(img.shape)\n",
    "#         yield img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nor = [0,0,0]\n",
    "# tum = [255,255,255]\n",
    "\n",
    "# COLOR_DICT = np.array([nor, tum])\n",
    "\n",
    "# def labelVisualize(num_class,color_dict,img):\n",
    "#     img = img[:,:,0] if len(img.shape) == 3 else img\n",
    "#     img_out = np.zeros(img.shape + (3,))\n",
    "#     for i in range(num_class):\n",
    "#         img_out[img == i,:] = color_dict[i]\n",
    "#     return img_out\n",
    "\n",
    "\n",
    "# def saveResult(save_path,npyfile,flag_multi_class = False,num_class = 2):\n",
    "#     for i,item in enumerate(npyfile):\n",
    "#         img = labelVisualize(num_class,COLOR_DICT,item) if flag_multi_class else item[:,:,0]\n",
    "#         img = cv2.resize(img, (64, 64))\n",
    "#         cv2.imwrite(os.path.join(save_path,\"%d_predict.tiff\"%i),img*255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testGene = testGenerator(\"E:\\\\deeplearning\\\\Hepatocarcinomes\\\\data\\\\5x\\\\training\\\\split64_image\\\\test\", target_size = (256,256))\n",
    "# results_test = model.predict_generator(testGene,7,verbose=1)\n",
    "# # print(sum(results[:,:,:,:]!=0))\n",
    "# # for i in range(7):\n",
    "# #     cv2.imwrite(\"E:\\\\deeplearning\\\\Hepatocarcinomes\\\\data\\\\5x\\\\training\\\\split64_image\\\\test\"+'\\\\'+str(i)+'.tiff',results[i]*255)\n",
    "# saveResult(\"E:\\\\deeplearning\\\\Hepatocarcinomes\\\\data\\\\5x\\\\training\\\\split64_image\\\\test\",results_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python r-tensorflow",
   "language": "python",
   "name": "r-tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
